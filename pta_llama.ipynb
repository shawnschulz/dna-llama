{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a4c38e-3899-4a17-9ae3-1caa2dcf92fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47f949-73f3-4f8f-b12b-788ace727bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f76b2-974e-4fae-8ca3-3fa47e4810ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84811750-48d7-4bbe-a62e-349650d4f469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcf4e2-8c22-444b-a4a4-9d0cfa3ac8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a49e5b-64ac-4000-9347-4cde66755b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install HTSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071c565-68a9-41f5-9dd0-f2eee269f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install enformer-pytorch>=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08cd7ff-ef1f-46cc-99d5-386b71018d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2debb38-e289-4093-8237-544cad1b6395",
   "metadata": {},
   "source": [
    "## Load modules and variable declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8585ddc7-8e8a-4674-9609-0d31e7c7e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dnaDataSet import dnaDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9923688e-5f24-4f0f-8efa-34522c8d7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "from enformer_pytorch import Enformer, GenomeIntervalDataset\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import json \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0bfa2-f9f6-44fc-9f55-c3e7ba22fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e148c112-2830-4546-a8ea-c283a3039703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can change the model path to any quantized model binary\n",
    "#TODO - make a script version of this with option for model path, relative context length, tsv_path, bam_path\n",
    "modelPath=\"/home/shawn/Programming/ai_stuff/llama.cpp/models/30B/ggml-model-q4_0.bin\"\n",
    "memoryDir=\"/home/shawn/datasets/llm_memory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32badd5-d1b6-428f-94a0-afa27e5234a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9cc79-9b6f-46fe-94e0-5c3ffa1d4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_dir= \"/home/shawn/datasets/enformer_data\"\n",
    "bed_path = \"/scratch/users/sschulz/pta_on_normal/chr10.bed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ce49a-d309-43e2-947c-cf5d6f4210de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#good testing but use a gvcf instead containing all known mutations first\n",
    "tsv_path = tsv_dir + \"/CARTPt04_Scan2_svc_merged_extract_snp.hg38_multianno.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85527b8e-22ef-447b-a239-a5c76b6ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv = pd.read_table(tsv_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08209602-e40c-4df6-a403-887fd5a14069",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv['CHROM'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac882d7-742f-46ee-af41-59474f5bc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_path=tsv_dir + '/CART-MRD-BALL-PTA-NEXTERA-WGS-CCT5007Pt04-D4_S26.realigned_deduped_sorted.bam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88babbeb-1cae-431f-ae04-812d235fe189",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## get the gvcf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f66973-b4a6-4f18-997b-8607dbf0dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sav_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5346907-66fb-4495-8e04-75486e6393d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gvcf_dir = '/scratch/users/sschulz/pta_on_normal/gvcf'\n",
    "download_log_dir='scratch/users/sschulz/pta_on_normal/gvcf/logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4f262-2392-49d6-86ae-26d64e07e3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(gvcf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062284c-eb35-40a5-af7e-6e4ed1bf2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_list = [chrom for sublist in [('MT', 'X', 'Y'), list(range(1,23))] for chrom in sublist]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ac3a2-c895-4781-bd0b-0e41b531ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadEnsembleGVCFs(output_dir, download_log_dir):\n",
    "    sav_dir = os.getcwd()\n",
    "    os.chdir(output_dir)\n",
    "    for i in [chrom for sublist in [('MT', 'X', 'Y'), list(range(1,23))] for chrom in sublist]:\n",
    "        command_ending = str(chrom) + '.gvf.gz'\n",
    "        print(f\"sbatch -c 2 --mem=32G -p cgawad --out={download_log_dir} --wrap='wget https://ftp.ensembl.org/pub/release-109/variation/gvf/homo_sapiens/homo_sapiens_incl_consequences-chr{i}.gvf.gz'\")\n",
    "        !sbatch -c 2 --mem=32G -p cgawad --out=$download_log_dir --wrap=f\"wget https://ftp.ensembl.org/pub/release-109/variation/gvf/homo_sapiens/homo_sapiens_incl_consequences-chr{i}.gvf.gz\"\n",
    "    os.chdir(sav_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917befbb-5687-429b-8c7a-b4c185d9f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadEnsembleGVCFs(gvcf_dir, download_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60072298-841f-4353-abd3-b14347740268",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ('MT', 'X', 'Y'):\n",
    "    print(f\"sbatch -c 2 --mem=32G -p cgawad --out='/scratch/users/sschulz/pta_on_normal/gvcf/logs/' --wrap='wget https://ftp.ensembl.org/pub/release-109/variation/gvf/homo_sapiens/homo_sapiens_incl_consequences-chr{i}.gvf.gz'\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7171e36-c62a-4c1d-8ff4-1ffa001dbe04",
   "metadata": {},
   "source": [
    "## Function and class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42f06b-ab0b-4ab1-8841-e43a99084677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is all in the dnaDataSet python file but I've kept it here just in case lol\n",
    " class dnaDataSet:\n",
    "    def save(self, fp):\n",
    "        '''\n",
    "            save dnaDataSet as pickle somewhere\n",
    "        '''\n",
    "        file_name = fp\n",
    "        with open(file_name, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "            print(f'dnaDataSet object successfully saved to \"{file_name}\"')\n",
    "    \n",
    "    def __init__(self, modelPath=False, memoryDir=os.getcwd()):\n",
    "        self.mutationDictionary={}\n",
    "        self.bamsDictionary={}\n",
    "        self.tsv=pd.DataFrame()\n",
    "        self.relativeContextLength=4\n",
    "        self.memoryDir=memoryDir\n",
    "        #promptOutput is a string formatted as a hf dataset\n",
    "        self.promptOutput=''\n",
    "        self.modelPath=modelPath\n",
    "    \n",
    "    def __getitem__(self, position):\n",
    "        '''\n",
    "        '''\n",
    "        items=[\n",
    "            self.mutationDictionary,\n",
    "            self.bamsDictionary,\n",
    "            self.tsv,\n",
    "            self.relativeContextLength,\n",
    "            self.memoryDir,\n",
    "            #promptOutput is a string formatted as a hf dataset\n",
    "            self.promptOutput\n",
    "        ]\n",
    "        return items[position]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "            \n",
    "        '''\n",
    "        return f'dnaDataSet object\\n current model being used: {str(self.modelPath)},\\n mutationDictionary: {str(self.mutationDictionary)},\\n bamsDictionary: {str(self.bamsDictionary)},\\n tsv: {str(self.tsv)},\\n relativeContextLength: {str(self.relativeContextLength)},\\n memoryDir: {str(self.memoryDir)},\\n promptOutput: {str(self.promptOutput)}'\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        '''\n",
    "            returns a dnaDataset with consolidated mutationDictionary and bamsDictionary, however other info is kept from the first dictionary\n",
    "        '''\n",
    "        selfCopy = self\n",
    "        otherCopy = other\n",
    "        selfCopy.mutationDictionary.update(otherCopy.mutationDictionary)\n",
    "        selfCopy.bamDictionary.update(otherCopy.mutationDictionary)\n",
    "        return selfCopy\n",
    "    \n",
    "    def __len__(self):\n",
    "        '''\n",
    "            prints the length of the mutationDictioanry keys\n",
    "        '''\n",
    "        print(\"The length of the mutationDictionary keys is :\")\n",
    "        return(len(self.mutationDictionary))\n",
    "    \n",
    "    def setRelativeContextLength(self, contextLength):\n",
    "        '''\n",
    "            takes int contextLength and sets it in the dataset\n",
    "        '''\n",
    "        self.relativeContextLength=contextLength\n",
    "    \n",
    "    def saveOutput(self, fp, memoryDir=False):\n",
    "        '''\n",
    "            saves the output of a prompting to memoryDir by default (so it can be used automtically when calling prompting), but can also be called\n",
    "            to save where user specifies filepath\n",
    "        '''\n",
    "        if not memoryDir:\n",
    "            self.promptOutput.save_to_disk(fp)\n",
    "        else:\n",
    "            self.promptOutput.save_to_disk(self.memoryDir + '/' + fp)\n",
    "    \n",
    "    def saveMutationDictionary(self, fp, memoryDir=False):\n",
    "        '''\n",
    "            saves mutationDictionary produced from tsv file and bam files to memoryDir by default as json file, but can also be called to save where user\n",
    "            specifies filepath\n",
    "        '''\n",
    "        if not memoryDir:\n",
    "            with open(fp, \"w\") as outfile:\n",
    "                json.dump(self.mutationDictionary, outfile)\n",
    "        else:\n",
    "            with open(self.memoryDir + '/' + fp, \"w\") as outfile:\n",
    "                json.dump(self.mutationDictionary, outfile)\n",
    "    \n",
    "    def makeLlamaDataset(self, tsv_dir, bam_path, bed_path):\n",
    "        '''\n",
    "            from a directory containing an annotated tsv file, many bam files and a bed path, create a huggingface dataset for use in llama\n",
    "\n",
    "            start by just passing lines from vcf to llama for fine tuning, along with a line that says \n",
    "            \"The read/basepairs/sequence at this position is:\n",
    "            The read information from reference is:\"\n",
    "\n",
    "            This is a pretty brute force way to do it but maybe it'll create something coherent from llama.\n",
    "\n",
    "\n",
    "            Getting correct sequence instruction: \n",
    "            \"instruction\": f\"The gene {gene} is mutated at the {start_pos} basepair. What is the sequence? What is the mutation?\",\n",
    "            \"input\": f\"{read_seq}\",\n",
    "            \"output\": \"5\"\n",
    "\n",
    "            Getting whether exonic or not/amino acid change:\n",
    "\n",
    "\n",
    "            [WIP] Instrucitons incorporating answers from databases:\n",
    "\n",
    "            Clinvar:\n",
    "\n",
    "            NCBI:\n",
    "\n",
    "            Genecards: \n",
    "\n",
    "        '''\n",
    "        for filename in os.listdir(tsv_dir):\n",
    "            if filename.endswith('tsv'):\n",
    "                tsv_file = os.path.join(tsv_dir, filename)\n",
    "                tsv_length=len(tsv_file)\n",
    "                counter = 0\n",
    "                print(\"the tsv file is: \")\n",
    "                print(tsv_file)\n",
    "                for i in range(tsv_length):\n",
    "                    chrom = tsv['CHROM'][i]\n",
    "                    start_pos = tsv['POS'][i]\n",
    "                    sample = tsv['SAMPLE'][i]\n",
    "                    gene = tsv['Gene.refGene'][i]\n",
    "                    gt = tsv['GT'][i]\n",
    "                    alt = tsv['ALT'][i]\n",
    "         #           print(\"tsv from the tsv file is: \")\n",
    "          #          print(' '.join(tsv.columns))\n",
    "                    if gt == '0/1' or gt == '1/1':\n",
    "                        print(start_pos)\n",
    "                        print(sample, gt)\n",
    "                        print(alt)\n",
    "                        print(gene)\n",
    "\n",
    "                        ### position of mutation is the position is says on the pileup - start position (0 indexed)\n",
    "                        ## start position can be greater than or less than position of read start, but luckily\n",
    "                        ## should be able to index the base that's changed either way \n",
    "\n",
    "                        #\n",
    "\n",
    "                        samfile = pysam.AlignmentFile(bam_path, \"rb\" )\n",
    "                        self.bamsDictionary[bam_path] = samfile\n",
    "                        pileup = samfile.pileup(chrom, start_pos, start_pos+1, min_mapping_quality=58)\n",
    "                        for read in pileup:\n",
    "                            read_list = str(read).split('\\t')\n",
    "                            read_start = read_list[5]\n",
    "                            read_seq = read_list[11]\n",
    "\n",
    "                            mutated_base= read_seq[int(read_start) - start_pos] \n",
    "\n",
    "\n",
    "                            print(f\"the start pos from tsv is {start_pos} the start pos from pileup is {read_start} the the gene is: \"+ gene +  ' the read is: ' + str(read_list) + ' and the mutated base is: ' + mutated_base)\n",
    "                            print('for sanity, the mutated allele was: ' + alt)\n",
    "                            self.mutationDictionary[\"Reference Genome: hg38, Read: \" + read_seq] =  f\"the start pos from tsv is {start_pos} the start pos from pileup is {read_start} the the gene is: \"+ gene + ' and the mutated base is: ' + mutated_base\n",
    "                    # for x in pileup:\n",
    "                    #     if counter == 0:\n",
    "                    #         print(str(x))\n",
    "\n",
    "        return(mutation_dictionary)\n",
    "    def fewShotLearning(self, read):\n",
    "        '''\n",
    "            takes a read as a prompt\n",
    "        '''\n",
    "        counter = 0\n",
    "        prompt_string = ''\n",
    "        for key in self.mutationDictionary.keys():\n",
    "            counter += 1\n",
    "            if counter < self.relativeContextLength:\n",
    "                prompt_string += \"Input: \" + key + \"\\n\" + \" Output: \" + self.mutationDictionary[key] + \"\\n\"\n",
    "        prompt = 'Reference Genome: hg38, Read: ' + read\n",
    "        output = llm(prompt_string + \"\\n\" + \"Input: \" + prompt + \"\\n\" + \"Output: \", max_tokens=32, stop=[\"Input:\"], echo=True)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aefe47-2062-4751-83fd-d20c1176463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLlamaDataset(tsv_dir, bam_path, bed_path):\n",
    "    '''\n",
    "        from a directory containing many annotated tsv files and a bed path, create a huggingface dataset for use in llama\n",
    "        \n",
    "        start by just passing lines from vcf to llama for fine tuning, along with a line that says \n",
    "        \"The read/basepairs/sequence at this position is:\n",
    "        The read information from reference is:\"\n",
    "        \n",
    "        This is a pretty brute force way to do it but maybe it'll create something coherent from llama.\n",
    "        \n",
    "        \n",
    "        Getting correct sequence instruction: \n",
    "        \"instruction\": f\"The gene {gene} is mutated at the {start_pos} basepair. What is the sequence? What is the mutation?\",\n",
    "        \"input\": f\"{read_seq}\",\n",
    "        \"output\": \"5\"\n",
    "        \n",
    "        Getting whether exonic or not/amino acid change:\n",
    "        \n",
    "        \n",
    "        [WIP] Instrucitons incorporating answers from databases:\n",
    "        \n",
    "        Clinvar:\n",
    "        \n",
    "        NCBI:\n",
    "        \n",
    "        Genecards: \n",
    "        \n",
    "    '''\n",
    "    mutation_dictionary = {}\n",
    "    for filename in os.listdir(tsv_dir):\n",
    "        if filename.endswith('tsv'):\n",
    "            tsv_file = os.path.join(tsv_dir, filename)\n",
    "            tsv_length=len(tsv_file)\n",
    "            counter = 0\n",
    "            print(\"the tsv file is: \")\n",
    "            print(tsv_file)\n",
    "            for i in range(tsv_length):\n",
    "                chrom = tsv['CHROM'][i]\n",
    "                start_pos = tsv['POS'][i]\n",
    "                sample = tsv['SAMPLE'][i]\n",
    "                gene = tsv['Gene.refGene'][i]\n",
    "                gt = tsv['GT'][i]\n",
    "                alt = tsv['ALT'][i]\n",
    "     #           print(\"tsv from the tsv file is: \")\n",
    "      #          print(' '.join(tsv.columns))\n",
    "                if gt == '0/1' or gt == '1/1':\n",
    "                    print(start_pos)\n",
    "                    print(sample, gt)\n",
    "                    print(alt)\n",
    "                    print(gene)\n",
    "                    \n",
    "                    ### position of mutation is the position is says on the pileup - start position (0 indexed)\n",
    "                    ## start position can be greater than or less than position of read start, but luckily\n",
    "                    ## should be able to index the base that's changed either way \n",
    "                    \n",
    "                    #\n",
    "                    \n",
    "                    samfile = pysam.AlignmentFile(bam_path, \"rb\" )\n",
    "                    pileup = samfile.pileup(chrom, start_pos, start_pos+1, min_mapping_quality=58)\n",
    "                    for read in pileup:\n",
    "                        read_list = str(read).split('\\t')\n",
    "                        read_start = read_list[5]\n",
    "                        read_seq = read_list[11]\n",
    "                        \n",
    "                        mutated_base= read_seq[int(read_start) - start_pos] \n",
    "                        \n",
    "                \n",
    "                        print(f\"the start pos from tsv is {start_pos} the start pos from pileup is {read_start} the the gene is: \"+ gene +  ' the read is: ' + str(read_list) + ' and the mutated base is: ' + mutated_base)\n",
    "                        print('for sanity, the mutated allele was: ' + alt)\n",
    "                        mutation_dictionary[\"Reference Genome: hg38, Read: \" + read_seq] =  f\"the start pos from tsv is {start_pos} the start pos from pileup is {read_start} the the gene is: \"+ gene + ' and the mutated base is: ' + mutated_base\n",
    "                # for x in pileup:\n",
    "                #     if counter == 0:\n",
    "                #         print(str(x))\n",
    "\n",
    "    return(mutation_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d053bf-dc37-4730-9fa9-327eb3738752",
   "metadata": {},
   "source": [
    "## test stuff with the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec835f2-9b1e-4d54-8bb8-1c842375ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnaset = dnaDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01afb7f0-fb80-44b5-921f-1353839132dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dnaDataSet object\n",
       " current model being used: False,\n",
       " mutationDictionary: {},\n",
       " bamsDictionary: {},\n",
       " tsv: Empty DataFrame\n",
       "Columns: []\n",
       "Index: [],\n",
       " relativeContextLength: 4,\n",
       " memoryDir: /oak/stanford/groups/cgawad/Scripts/dna-llama,\n",
       " promptOutput: "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnaset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770d0dfc-f77b-490d-8ff9-656b8f28c566",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelPath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dnaset2\u001b[38;5;241m=\u001b[39mdnaDataSet(modelPath\u001b[38;5;241m=\u001b[39m\u001b[43mmodelPath\u001b[49m, memoryDir\u001b[38;5;241m=\u001b[39mmemoryDir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelPath' is not defined"
     ]
    }
   ],
   "source": [
    "dnaset2=dnaDataSet(modelPath=modelPath, memoryDir=memoryDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0a7d5-3c10-4f49-8b6b-1d9c9182ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnaset2.save(\"/scratch/users/sschulz/dnaset.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03972b0-2f80-4104-9542-bc9f14746e59",
   "metadata": {},
   "source": [
    "## Few shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d690a-c687-4ece-9e9b-d78345705c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Idea was to finetune with the dataset form makeLlamaDataset, but for now we are just trying to use it to do few shot learning by taking some examples\n",
    "    from it and using it to get it to tell you the mutated base in a read you give it\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77160857-93c0-4a99-851e-a846c592c0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dnaset2.makeLlamaDataset(tsv_dir, bam_path, bed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df6dea-f5d2-4ad3-83dc-4c96bed2145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnaset2.fewShotLearning('TAGAAAATGTGGATGAGACCTTCTGCATAGATAACGAAGCGCTATATGACATATGTTCCAGGACCCTAAAACTGCCCACACCCACCTATGGTGACCTGAA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97f5a10b",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b6dc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/shawn/.cache/huggingface/datasets/pollner___parquet/pollner--dna_dataset-df9452f1a4694811/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1115.70it/s]\n"
     ]
    }
   ],
   "source": [
    "modelPath='/home/shawn/datasets/LLMs/llama_7b/config.json'\n",
    "memoryDir='/home/shawn/datasets'\n",
    "trainingDataset='pollner/dna_dataset' \n",
    "fineTuningDataset=dnaDataSet(modelPath=modelPath, memoryDir=memoryDir)\n",
    "fineTuningDataset.mutationDictionary = load_dataset(trainingDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b051d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dnaDataSet object\n",
       " current model being used: /home/shawn/datasets/LLMs/llama_7b/config.json,\n",
       " mutationDictionary: DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['target', 'context'],\n",
       "        num_rows: 12844\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['target', 'context'],\n",
       "        num_rows: 1606\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['target', 'context'],\n",
       "        num_rows: 1605\n",
       "    })\n",
       "}),\n",
       " bamsDictionary: {},\n",
       " tsv: Empty DataFrame\n",
       "Columns: []\n",
       "Index: [],\n",
       " relativeContextLength: 4,\n",
       " memoryDir: /home/shawn/datasets,\n",
       " promptOutput: "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fineTuningDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cee4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "  warn(msg)\n",
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/2021,unix/bankerz-tower'), PosixPath('local/bankerz-tower')}\n",
      "  warn(msg)\n",
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-ubuntu')}\n",
      "  warn(msg)\n",
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/shawn/.local/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:07<00:00,  4.39it/s]\n",
      "  0%|          | 0/9633 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 12684 is out of bounds for size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m fineTuningDataset\u001b[39m.\u001b[39;49mfinetune()\n",
      "File \u001b[0;32m~/Programming/ai_stuff/dna-llama/dnaDataSet.py:232\u001b[0m, in \u001b[0;36mdnaDataSet.finetune\u001b[0;34m(self, model, dataset, micro_batch_size, num_epochs, learning_rate, group_by_length, output_dir)\u001b[0m\n\u001b[1;32m    208\u001b[0m     tokenizer\u001b[39m.\u001b[39mpadding_side \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Allow batched inference\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     trainer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m    211\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    212\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrainingDataSet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     ),\n\u001b[1;32m    230\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    233\u001b[0m     model\u001b[39m.\u001b[39msave_pretrained(output_dir)\n\u001b[1;32m    234\u001b[0m     \u001b[39mreturn\u001b[39;00m(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1899\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1899\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1900\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1901\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2713\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, keys: List) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m   2712\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2713\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(keys)\n\u001b[1;32m   2714\u001b[0m     n_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(batch))])\n\u001b[1;32m   2715\u001b[0m     \u001b[39mreturn\u001b[39;00m [{col: array[i] \u001b[39mfor\u001b[39;00m col, array \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2709\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2709\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2693\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2692\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2693\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2694\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2695\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2696\u001b[0m )\n\u001b[1;32m   2697\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:588\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n\u001b[0;32m--> 588\u001b[0m     _check_valid_index_key(key, size)\n\u001b[1;32m    589\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:541\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Iterable):\n\u001b[1;32m    540\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(key) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 541\u001b[0m         _check_valid_index_key(\u001b[39mint\u001b[39;49m(\u001b[39mmax\u001b[39;49m(key)), size\u001b[39m=\u001b[39;49msize)\n\u001b[1;32m    542\u001b[0m         _check_valid_index_key(\u001b[39mint\u001b[39m(\u001b[39mmin\u001b[39m(key)), size\u001b[39m=\u001b[39msize)\n\u001b[1;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:531\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m    530\u001b[0m     \u001b[39mif\u001b[39;00m (key \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m key \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[0;32m--> 531\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    532\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Invalid key: 12684 is out of bounds for size 0"
     ]
    }
   ],
   "source": [
    "model = fineTuningDataset.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76286dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
