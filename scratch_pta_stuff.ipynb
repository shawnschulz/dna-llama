{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c5ad3e-61b1-40b2-935a-302b00d17391",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1116fbf-c601-4e08-a887-feea0f35f8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb82898-2750-49a6-ac6f-d867a74bbbe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6d297-8360-4ad6-8fbf-ec2c21876f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085e7e4-b4af-49a2-b6a8-a8e9160dc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceae258-8408-4d52-a3b5-711cdaad413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install HTSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f2e52-f880-4ed2-acd1-e584f1564d2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LLAMA loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fe8e1-3942-4c41-a564-3b3f0d3d4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265e7f5-5fa9-4fb7-89a7-b296985e9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    path on clust to convert weights is:\n",
    "    path on clust to convert tokenizer:\n",
    "\"\"\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97d04e-d88f-43a9-952d-8c3eaf322bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Tutorial Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588d858-18de-4135-8a46-34709aa3f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c2569-8e37-4521-8371-ff51214d20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91268d9c-f157-4ab1-a196-0888ccae7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0c43f-6862-4f4d-ae38-42bf4b76179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6e3d9-891a-469c-a52d-b0960561f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a6daa-9f4b-4487-9be5-35fcaf4c2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"ACGTGGTATGATGATAGATGATGA\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e372d8-b4b8-46f5-8dd6-e155be2143af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9c39a-07c2-4899-b1c4-4eec87badcb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading dna sequences from a BAM and creating a dataset for fine tuning Llama to understand sequence positions and mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927e087-7c8b-4816-b420-065cfaaaeef7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Attempt with HTSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d508e2-3819-4134-b1eb-fa83f4ef41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HTSeq\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8c9d2-8cde-491a-b936-09ea9a4fd46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a91376-d7b9-4cf2-9664-0ec5b707187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir = \"/oak/stanford/groups/cgawad/Reference_Files/GATK_Resource_Bundle_hg38/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0bd8b-e3bb-41d7-bb2a-2ba504eb5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### change this later to be inputted \n",
    "gtf_file = HTSeq.GFF_Reader(ref_dir + \"hg38.refGene.gtf.gz\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11e5d9-3047-4db2-be9b-ebd7f5d76de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg38_fasta = ref_dir + \"Homo_sapiens_assembly38.fasta\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbd409-7867-4001-9b6f-5ec5f051dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### using htseq's fasta reader, get ref genome hg38 sequences and put in a good format for the dataset\n",
    "\n",
    "def addRef(fasta_dir):\n",
    "    '''\n",
    "        Given a directory pointing to a genome reference fasta, return a json file or hf dataset object with the following info:\n",
    "            1. iterate through the fasta and get sequence info\n",
    "            2. for each read want to have the chr #, description, and whther it is in exonic portion of a gene\n",
    "                -if not exonic, ideally want what type non-coding element the read is a part of\n",
    "            3. NA (or whatever is best) for Mutation, Clinvar, Cosmic\n",
    "            4. desc of gene from NCBI \n",
    "    '''\n",
    "    data_dict = {}\n",
    "    for read in HTSeq.FastaReader(fasta_dir):\n",
    "        ### may make more sense to iterate through gtf file instead of fasta, but we need to iterate through the sequences somehow\n",
    "        ### and also iterate through the read nammes with chrom position etc, \n",
    "        chr_pos = #a string with the chromosome number, start pos and end position\n",
    "        data_dict[chr_pos] = [\n",
    "            {\n",
    "                'seq':read,\n",
    "                'chr':#the chromosome #\n",
    "                'pos':#preferably the start AND end position\n",
    "                'refGene':#gene name or whether intronic, ncrna, splicing etc\n",
    "                'NCBI':#info from ncbi database\n",
    "                'mutation':'NA',\n",
    "                'clinvar':'NA',\n",
    "                'cosmic':'NA'\n",
    "            }\n",
    "        ]\n",
    "        print(\"Sequence '%s' has length %d.\" % ( s.name, len(s) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b9359-6d3a-4009-bd17-d32513175709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def addBAM(bam_path, vcf_path):\n",
    "    '''\n",
    "        Given a path pointing ot a bam file, return a json or hf dataset object with following info:\n",
    "            1. iterate through the bam file and get sequence info on reads containing mutations appearing in vcf file\n",
    "            2. for each read want to have the chr #, description, and whther it is in exonic portion of a gene\n",
    "                -if not exonic, ideally want what type non-coding element the read is a part of\n",
    "            3. If a mutation, want info for Mutation from Clinvar, Cosmic, NCBI\n",
    "            4. desc of gene from NCBI \n",
    "            \n",
    "            desc of sam_alignment from htseq:\n",
    "                >>> aln.iv\n",
    "                <GenomicInterval object 'IV', [246048,246084), strand '+'>\n",
    "                >>> aln.iv.chrom\n",
    "                'IV'\n",
    "                >>> aln.iv.start\n",
    "                246048\n",
    "                >>> aln.iv.end\n",
    "                246084\n",
    "                >>> aln.iv.strand\n",
    "                '+'\n",
    "    '''\n",
    "    data_dict = {}\n",
    "    with HTSeq.BAM_Reader(bam_path) as f:\n",
    "        for i, sam_alignment in enumerate(f):\n",
    "            ### did this a alittle backwards, should iterate through gtf file to get gene name and interval, then (hopefully) use that\n",
    "            ### to index into the bam file to get the sequence rather than iterating through all parts of the bam file\n",
    "            if sam_alignment.aligned == True:\n",
    "                ### i'm not sure what's gonna be the most useful thing for llama to map all the info to, starting with a string with\n",
    "                ### chrom and pos and what file it's from\n",
    "                chrom_pos_identifier = sam_alignment.iv.chrom + ' START: ' + sam_alignment.iv.start + ' END: ' + sam_alignment.iv.end \n",
    "                data_dict[chrom_pos_identifier] = [\n",
    "                {\n",
    "                    'read_name':sam_alignment.read.name\n",
    "                    'seq':sam_alignment.read,\n",
    "                    'chr':sam_alignment.iv.chrom\n",
    "                    'pos':sam_alignment.iv.start + '_' + sam_alignment.iv.end #preferably the start AND end position\n",
    "                    'refGene': #gene name or whether intronic, ncrna, splicing etc\n",
    "                    'NCBI': #info from ncbi database\n",
    "                    'mutation':'NA',\n",
    "                    'clinvar':'NA',\n",
    "                    'cosmic':'NA'\n",
    "                }\n",
    "                ]\n",
    "            print(read)\n",
    "            ## for testing don't do the whole thing\n",
    "            if i == 2:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976be9d1-e81b-4b86-9b09-228a85e6de94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exons = HTSeq.GenomicArrayOfSets( \"auto\", stranded=True )\n",
    "# counter = 0\n",
    "# for feature in gtf_file:\n",
    "#     if counter <= 5:\n",
    "#         print(feature)\n",
    "#         if feature.type == \"exon\":\n",
    "#            exons[ feature.iv ] += feature.attr[\"gene_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65525a9-e36a-455f-ad1e-9f9d39e2ed21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Attempt with pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b695b-3683-41bd-b3e8-76c2f54dcfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09ed9e-9508-4abe-a817-0607b526d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f946c4-b6a5-4a2b-9a12-cd8c6b2c3216",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bam=\"/scratch/users/sschulz/CARTPt04_Scan2/CART-MRD-BALL-PTA-NEXTERA-WGS-CCT5007Pt04-A8_S50.recalibrated_realigned_deduped_sorted.bam\"\n",
    "#chrom is formatted chr1, chr2, chr3 etc. for hg38, may be diff for other ones\n",
    "chrom='chr1'\n",
    "start_pos=0\n",
    "end_pos=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b014a7-2fd4-4a57-aef5-22c1d45693db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the normal samtools command we want to run: samtools view CART-MRD-BALL-PTA-NEXTERA-WGS-CCT500*.bam chr1:1322100-1332100 | awk '{if($1 !~ /^@/) print $10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aadc5a-9527-4b62-ae09-7411d2c99894",
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile = samfile = pysam.AlignmentFile(input_bam, \"rb\")\n",
    "iter = samfile.fetch(\"chr1\")\n",
    "temp = []\n",
    "for x in iter:\n",
    "    temp.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52dec8d-0145-4af4-bd60-3c2689835e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## idfk how pysam works tbh\n",
    "header = tk_bam.get_bam_header_as_dict(input_bam)\n",
    "\n",
    "bam = pysam.Samfile(input_bam)\n",
    "\n",
    "for rec in bam:\n",
    "    # Convert to string and back to replace the old tid with the new one\n",
    "    # This appears to be the only way to do this with pysam (sometime after 0.9)\n",
    "    rec = pysam.AlignedSegment.fromstring(rec.to_string(),\n",
    "                                          header=pysam.AlignmentHeader.from_dict(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e2429-e5a1-41b4-a1ad-e41ba0a90434",
   "metadata": {},
   "outputs": [],
   "source": [
    "## idk this isn't working\n",
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# # Specify the input and output directories\n",
    "# input_dir = \"/scratch/users/sschulz/CARTPt04_Scan2/\"\n",
    "# output_dir = \"/scratch/users/sschulz/CARTPt04_Scan2/\"\n",
    "\n",
    "# # Iterate over the files in the input directory\n",
    "# for filename in os.listdir(input_dir):\n",
    "#     # Check if the file is a SAM file\n",
    "#     if filename.endswith(\".sam\"):\n",
    "#         # Construct the input and output file paths\n",
    "#         input_file = os.path.join(input_dir, filename)\n",
    "#         output_file = os.path.join(output_dir, filename.replace(\".sam\", \"_gene_seq.txt\"))\n",
    "        \n",
    "#         # Run the samtools and awk commands using subprocess\n",
    "#         with open(output_file, \"w\") as f:\n",
    "#             subprocess.run(['ml', 'load', 'biology', 'samtools'], stdout=subprocess.PIPE)\n",
    "#             subprocess.run([\"samtools\", \"view\", input_file], stdout=subprocess.PIPE)\n",
    "#             subprocess.run([\"awk\", \"{if($1 !~ /^@/) print $10}\"], stdin=process.stdout, stdout=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454eb09-86de-4d66-ba20-3566f7192b5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading DNA sequence data from BAMs to create dataset for fine tuning DNABERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4036c-0b2e-4fe1-b898-80172260c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HTSeq\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc15916-ad0c-4e71-b6f2-748fce9ffede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareBertDataset(bam_file, vcf_file):\n",
    "    '''\n",
    "        get mutations from vcf file, then get the read containing that mutation from the bam file, then mask that mutation and format\n",
    "        for BERT (i.e. token with the mask at the mutation position so it can learn with PTA artifacts look like\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d9555e-ddfa-414c-97d6-18ee92269856",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DNA tokenizer Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a75ed6-81f5-4cce-9b67-f2178767f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AIRI-Institute/gena-lm-bert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2f48d-03fd-4271-8e32-2a155a5d450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(\"ACGTGGTATGATGATAGATGATGA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee442c-4a03-4a89-a14f-ca606ba062fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a08383-f1c8-4609-8b52-ee14a105f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab69da-04ca-4442-89a0-2d00f322fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sequences = [\n",
    "    \"ACGTAGCTGACTGACTTAGTGA\",\n",
    "    \"ACTAGCATGCATCGTAGCTAGCTAGACTGA\",\n",
    "    \"ATATATATTACACACACGAGACTAGCTT\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1c782-cd9f-4150-a6b4-596d00c0b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input=tokenizer(batch_sequences, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5b846-3692-4424-b49f-bccd5fb95e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701126e-e8d3-4fca-8e33-8aa6dc2bab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in encoded_input['input_ids']:\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce5d5e-6108-43ba-947e-8f75d9ee9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## p much above but we've padded, truncated (no maximum length provided tho) and returned tensors\n",
    "encoded_input = tokenizer(batch_sequences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52667f-29e9-4316-8659-110545deae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## so we can tokenize DNA sequences, but how do we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15f1ef-fe24-4401-bdb7-e18370237f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109a7b4-4e67-4ec0-9840-bc641b7b3ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b4ec2-2917-4e19-93d4-7c413a282c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
