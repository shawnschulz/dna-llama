{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5e25df-aca9-41c1-b9f4-e592b005f460",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fa7f8-5f83-40c3-9792-882c40b9312a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc3f67-cc87-4ac8-8ed1-d6a2ac1f10a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b69fb5-19ea-4cab-96b5-22fb99f04f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c502e4-3bbf-4106-ba39-08b36abb817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc08e69-7d8d-4368-805a-3d07968804f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install HTSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc79d57-331b-4894-b306-7ea092a926f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install enformer-pytorch>=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba1ede2-ccd6-433d-aed3-b7df13be183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Downloading polars-0.16.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0.1 in /home/groups/cgawad/conda_for_pf_notebook/miniconda3_for_pf/envs/pf/lib/python3.10/site-packages (from polars) (4.4.0)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-0.16.18\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4734ba7-c0d4-4f4e-8877-13a88bfcd174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/groups/cgawad/conda_for_pf_notebook/miniconda3_for_pf/envs/pf/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/groups/cgawad/conda_for_pf_notebook/miniconda3_for_pf/envs/pf/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/groups/cgawad/conda_for_pf_notebook/miniconda3_for_pf/envs/pf/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/groups/cgawad/conda_for_pf_notebook/miniconda3_for_pf/envs/pf/lib/python3.10/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/groups/cgawad/conda_for_pf_notebook/miniconda3_for_pf/envs/pf/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d7ef-b040-4419-b0d0-c4a8178e6b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Module imports and variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1079afb-7b2c-4833-bc76-621e0f397109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "from enformer_pytorch import Enformer, GenomeIntervalDataset\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5a7935-f1d4-402e-89d4-583fcba9ed3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### define variables\n",
    "hg38_bed_path = \"/scratch/users/sschulz/pta_on_normal/Homo_sapiens_assembly38_n25chr.bed\"\n",
    "bam_dir= \"/scratch/users/sschulz/pta_on_normal\"\n",
    "fasta_dir = bam_dir + '/fastas'\n",
    "dataset_name='test_dataset.hf'\n",
    "preprocess_bam=True\n",
    "vcf_file=bam_dir +'/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz'\n",
    "ref_fasta=bam_dir + '/Homo_sapiens_assembly38.fasta'\n",
    "results_dir=bam_dir\n",
    "output_fasta_fn='chr_test_fasta.fasta'\n",
    "sbatch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7229ab-99f5-4d74-a1e0-2a5cf0c47db1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366bf928-a137-44bb-b33e-39616a80d8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preProcessVCF(vcf_path, ref_fasta, results_dir, output_fasta_fn, sbatch):\n",
    "    '''\n",
    "        take directory of a combined vcf file with somatic mutations called for normal cells\n",
    "        with pta run on them and make a fasta file \n",
    "        \n",
    "        TO-DO: make something that generates the vcf for you if given normal bam files\n",
    "    '''\n",
    "\n",
    "    !sh vcf2fasta.sh --vcf_path $vcf_path --ref_fasta $ref_fasta --results_dir $results_dir --output_name $output_fasta_fn --sbatch $sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbddb491-ee04-4717-bceb-edd694f04f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEnformerDataset(fasta_path, bed_path):\n",
    "    '''\n",
    "        makes enformer dataset for genome intervals in bed file at bed_path, returns the dataset\n",
    "    '''\n",
    "    #filter_train = lambda df: df.filter(pl.col('column_4') == 'train')\n",
    "    ds = GenomeIntervalDataset(\n",
    "        bed_file = bed_path, ##<- this can just be the whole hg38.bed for all chromosomes\n",
    "        fasta_file = fasta_path,  ## path to fasta file\n",
    "        #filter_df_fn = filter_train,                        # filter dataframe function\n",
    "        return_seq_indices = False,                          # return nucleotide indices (ACGTN) or one hot encodings\n",
    "        shift_augs = (-2, 2),                               # random shift augmentations from -2 to +2 basepairs\n",
    "        rc_aug = True,                                      # use reverse complement augmentation with 50% probability\n",
    "        context_length = 196_608,\n",
    "        return_augs = True                                  # return the augmentation meta data\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a767409c-b1a4-4a25-9375-476c72ac0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEnformerPTA(dataset):\n",
    "    '''\n",
    "        should train the enformer model\n",
    "    '''\n",
    "    \n",
    "    #filter_train = lambda df: df.filter(pl.col('column_4') == 'train')\n",
    "\n",
    "    model = HeadAdapterWrapper(\n",
    "                enformer = dataset,\n",
    "                num_tracks = 128,\n",
    "                post_transformer_embed = False   # by default, embeddings are taken from after the final pointwise block w/ conv -> gelu - but if you'd like the embeddings right after the transformer block with a learned layernorm, set this to True\n",
    "            ).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce7b7d85-dea8-4e4e-b365-3ef6d9909738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOneHot(fastaFilePath, num_lines):\n",
    "    '''\n",
    "       Give path to a fasta file and the number of lines you want to turn into one hot encodings, get the one_hot encoded\n",
    "       tensor\n",
    "    '''\n",
    "    with open(fastaFilePath, 'r') as input_file:\n",
    "        try:\n",
    "            head = [next(input_file).strip('\\n') for _ in range(num_lines)]\n",
    "        except StopIteration as e:\n",
    "            print(\"Exceeded length of input file, stopping.\")\n",
    "    seq = str_to_one_hot(head)\n",
    "    input_file.close()\n",
    "    return(seq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be1b5d5-2505-4618-8e6c-0068f2c8850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this was all taken from lucidrains/enformer-pytorch, i have no clue if i could just source this somehow and i don't care\n",
    "\n",
    "def torch_fromstring(seq_strs):\n",
    "    def cast_list(t):\n",
    "        return t if isinstance(t, list) else [t]\n",
    "    batched = not isinstance(seq_strs, str)\n",
    "    seq_strs = cast_list(seq_strs)\n",
    "    np_seq_chrs = list(map(lambda t: np.fromstring(t, dtype = np.uint8), seq_strs))\n",
    "    seq_chrs = list(map(torch.from_numpy, np_seq_chrs))\n",
    "    return torch.stack(seq_chrs) if batched else seq_chrs[0]\n",
    "\n",
    "def str_to_one_hot(seq_strs):\n",
    "    seq_chrs = torch_fromstring(seq_strs)\n",
    "    return one_hot_embed[seq_chrs.long()]\n",
    "\n",
    "one_hot_embed = torch.zeros(256, 4)\n",
    "one_hot_embed[ord('a')] = torch.Tensor([1., 0., 0., 0.])\n",
    "one_hot_embed[ord('c')] = torch.Tensor([0., 1., 0., 0.])\n",
    "one_hot_embed[ord('g')] = torch.Tensor([0., 0., 1., 0.])\n",
    "one_hot_embed[ord('t')] = torch.Tensor([0., 0., 0., 1.])\n",
    "one_hot_embed[ord('n')] = torch.Tensor([0., 0., 0., 0.])\n",
    "one_hot_embed[ord('A')] = torch.Tensor([1., 0., 0., 0.])\n",
    "one_hot_embed[ord('C')] = torch.Tensor([0., 1., 0., 0.])\n",
    "one_hot_embed[ord('G')] = torch.Tensor([0., 0., 1., 0.])\n",
    "one_hot_embed[ord('T')] = torch.Tensor([0., 0., 0., 1.])\n",
    "one_hot_embed[ord('N')] = torch.Tensor([0., 0., 0., 0.])\n",
    "one_hot_embed[ord('.')] = torch.Tensor([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7e272-f2c9-42ef-9bcc-cf1dcb9fe831",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8293b8-faf6-4277-9cba-c42c52693b22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## preprocessing the target fasta\n",
    "bam_dir='/home/shawn/datasets/enformer_data'\n",
    "vcf_file=bam_dir +'/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz'\n",
    "ref_fasta=bam_dir + '/Homo_sapiens_assembly38.fasta'\n",
    "results_dir=bam_dir\n",
    "output_fasta_fn='final_fasta.fasta'\n",
    "preprocess_bam=False\n",
    "if preprocess_bam:\n",
    "    preProcessVCF(vcf_file, ref_fasta, results_dir, output_fasta_fn, sbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f89d49-d108-44f4-8d81-d38a9f1db215",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bam_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## preprocessing the input fasta\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vcf_file\u001b[39m=\u001b[39mbam_dir \u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      3\u001b[0m ref_fasta\u001b[39m=\u001b[39mbam_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Homo_sapiens_assembly38.fasta\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m results_dir\u001b[39m=\u001b[39mbam_dir\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bam_dir' is not defined"
     ]
    }
   ],
   "source": [
    "## preprocessing the input fasta\n",
    "vcf_file=bam_dir +'/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz'\n",
    "ref_fasta=bam_dir + '/Homo_sapiens_assembly38.fasta'\n",
    "results_dir=bam_dir\n",
    "output_fasta_fn='test_fasta.fasta'\n",
    "if preprocess_bam:\n",
    "    preProcessVCF(vcf_file, ref_fasta, results_dir, output_fasta_fn, sbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506a144-6c45-4ec9-abf4-5b6bdf94ec92",
   "metadata": {},
   "source": [
    "## get one-hot encodings with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03e8c54d-1b42-4d3b-b253-6045659d6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11198/318148619.py:8: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  np_seq_chrs = list(map(lambda t: np.fromstring(t, dtype = np.uint8), seq_strs))\n"
     ]
    }
   ],
   "source": [
    "lines_of_fasta_file=3000\n",
    "targetFastaFilePath = bam_dir + '/final_fasta.fasta'\n",
    "\n",
    "target = makeOneHot(targetFastaFilePath, lines_of_fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42c6475a-efb6-4ccf-a482-991fb45074c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11198/318148619.py:8: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  np_seq_chrs = list(map(lambda t: np.fromstring(t, dtype = np.uint8), seq_strs))\n"
     ]
    }
   ],
   "source": [
    "lines_of_fasta_file=3000\n",
    "inputFastaFilePath = bam_dir + '/test_fasta.fasta'\n",
    "\n",
    "input_seq = makeOneHot(targetFastaFilePath, lines_of_fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7b83a01-60c4-47b3-a023-448e3bc0ccb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_seq = input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbf89ffc-e6f7-4307-b2ea-f86974184bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18978368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 60, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape\n",
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7ae09-eb18-4b7d-8503-e1f9a37e8e63",
   "metadata": {},
   "source": [
    "## Run the model on new DNA tracks with somatic mutations detected in normal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a19efb5d-f255-47fa-9ecb-ade7f30d910c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (5313) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 27\u001b[0m\n\u001b[1;32m     14\u001b[0m output, embeddings \u001b[39m=\u001b[39m model(input_seq, return_embeddings \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m    target should EITHER be the one hot encodings of an input fasta with many artifacts, or the weights from \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m    just running the model on the one hot encodings without a target, not exactly sure which but try the \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m    another fasta that should have normal artifacts in it, and a single cell DNA sequencing sample\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss, embeddings \u001b[39m=\u001b[39m model(\n\u001b[1;32m     28\u001b[0m     input_seq,\n\u001b[1;32m     29\u001b[0m     head \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     target \u001b[39m=\u001b[39;49m target,\n\u001b[1;32m     31\u001b[0m     return_embeddings \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     36\u001b[0m \u001b[39m# after much training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/enformer_pytorch/modeling_enformer.py:443\u001b[0m, in \u001b[0;36mEnformer.forward\u001b[0;34m(self, x, target, return_corr_coef, return_embeddings, return_only_embeddings, head, target_length)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mif\u001b[39;00m return_corr_coef:\n\u001b[1;32m    441\u001b[0m         \u001b[39mreturn\u001b[39;00m pearson_corr_coef(out, target)\n\u001b[0;32m--> 443\u001b[0m     \u001b[39mreturn\u001b[39;00m poisson_loss(out, target)\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m return_embeddings:\n\u001b[1;32m    446\u001b[0m     \u001b[39mreturn\u001b[39;00m out, x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/enformer_pytorch/modeling_enformer.py:45\u001b[0m, in \u001b[0;36mpoisson_loss\u001b[0;34m(pred, target)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpoisson_loss\u001b[39m(pred, target):\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m (pred \u001b[39m-\u001b[39m target \u001b[39m*\u001b[39;49m log(pred))\u001b[39m.\u001b[39mmean()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (5313) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from enformer_pytorch import Enformer, seq_indices_to_one_hot\n",
    "\n",
    "# will need to figure out correct way to configure hyperparams\n",
    "model = Enformer.from_hparams(\n",
    "    dim = 1536,\n",
    "    depth = 11,\n",
    "    heads = 8,\n",
    "    output_heads = dict(human = 4), #<- tbh idk what the correct number of output heads should be\n",
    "    target_length = 1,\n",
    ")\n",
    "\n",
    "\n",
    "output, embeddings = model(input_seq, return_embeddings = True)\n",
    "\n",
    "'''\n",
    "    target should EITHER be the one hot encodings of an input fasta with many artifacts, or the weights from \n",
    "    just running the model on the one hot encodings without a target, not exactly sure which but try the \n",
    "    one hot encodigns first\n",
    "    \n",
    "    input_seq should be the one-hot embeddings of any fasta\n",
    "    \n",
    "    corr_coef should be higher if many artifacts, lower if not many artifacts. check against a bulk sequencing sample,\n",
    "    another fasta that should have normal artifacts in it, and a single cell DNA sequencing sample\n",
    "'''\n",
    "\n",
    "loss, embeddings = model(\n",
    "    input_seq,\n",
    "    head = 'human',\n",
    "    target = target,\n",
    "    return_embeddings = True\n",
    ")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# after much training\n",
    "\n",
    "corr_coef = model(\n",
    "    input_seq,\n",
    "    head = 'human',\n",
    "    target = target,\n",
    "    return_corr_coef = True\n",
    ")\n",
    "\n",
    "corr_coef # pearson R, used as a metric in the paper. For our first test, this should return something close to 1, since \n",
    "          # we are running it on the exact same fasta file as a sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1142a45-1dd7-4798-a956-12080533b04e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the model with finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ea8b29-7eab-4356-bab1-7d8a457542ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from enformer_pytorch import Enformer\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper\n",
    "\n",
    "enformer = Enformer.from_pretrained('EleutherAI/enformer-official-rough')\n",
    "\n",
    "model = HeadAdapterWrapper(\n",
    "    enformer = enformer,\n",
    "    num_tracks = 128,\n",
    "    post_transformer_embed = False   # by default, embeddings are taken from after the final pointwise block w/ conv -> gelu - but if you'd like the embeddings right after the transformer block with a learned layernorm, set this to True\n",
    ").cuda()\n",
    "\n",
    "target = torch.randn(1, 1, 128).cuda()  # 128 tracks\n",
    "\n",
    "loss = model(input_seq, target = target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd56850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadAdapterWrapper(\n",
       "  (enformer): Enformer(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_tower): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (crop_final): TargetLengthCrop()\n",
       "    (final_pointwise): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Rearrange('b d n -> b n d')\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): GELU()\n",
       "    )\n",
       "    (_trunk): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Rearrange('b d n -> b n d')\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TargetLengthCrop()\n",
       "      (6): Sequential(\n",
       "        (0): Rearrange('b n d -> b d n')\n",
       "        (1): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Rearrange('b d n -> b n d')\n",
       "        (3): Dropout(p=0.05, inplace=False)\n",
       "        (4): GELU()\n",
       "      )\n",
       "    )\n",
       "    (_heads): ModuleDict(\n",
       "      (human): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "        (1): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mouse): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=1643, bias=True)\n",
       "        (1): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_embed_transform): Sequential(\n",
       "    (0): Identity()\n",
       "  )\n",
       "  (to_tracks): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf4d915-3267-40f7-8b23-4cb1c49b82d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HeadAdapterWrapper.forward() got an unexpected keyword argument 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[39m=\u001b[39m model(input_seq, head \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# (3000, 3000)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: HeadAdapterWrapper.forward() got an unexpected keyword argument 'head'"
     ]
    }
   ],
   "source": [
    "pred = model(input_seq, head = 'human') # (3000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2ec0ffa-c886-4a84-ba4c-e05c65a60fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7276, 0.7898, 0.7651,  ..., 0.8316, 0.6573, 0.5948]],\n",
       "\n",
       "        [[0.7207, 0.7824, 0.8117,  ..., 0.7090, 0.5889, 0.5795]],\n",
       "\n",
       "        [[0.5686, 0.8286, 0.8646,  ..., 0.7037, 0.6471, 0.6555]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.7175, 0.7805, 0.7743,  ..., 0.7038, 0.8123, 0.5317]],\n",
       "\n",
       "        [[0.6933, 0.6910, 0.7489,  ..., 0.7125, 0.6352, 0.6702]],\n",
       "\n",
       "        [[0.6963, 0.7716, 0.8335,  ..., 0.6973, 0.6662, 0.6498]]],\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcc18d-6460-4a88-811b-904883888cdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d03666-0dca-4570-bffa-f849dba480d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat ‘/scratch/users/sschulz/pta_on_normal/fastas/*.fai’: No such file or directory\n",
      "Now collecting dataset for: /scratch/users/sschulz/pta_on_normal/fastas/CART-MRD-BALL-PTA-NEXTERA-WGS-CCT5007Pt04-A9_S57.realigned_deduped_sorted.fasta\n",
      "Now collecting dataset for: /scratch/users/sschulz/pta_on_normal/fastas/CART-MRD-BALL-PTA-NEXTERA-WGS-CCT5007Pt04-B10_S65.realigned_deduped_sorted.fasta\n"
     ]
    }
   ],
   "source": [
    "dataset_list = []\n",
    "fai_dir=fasta_dir + '/fais'\n",
    "!mkdir -p {fai_dir}\n",
    "for fasta_name in os.listdir(fasta_dir):\n",
    "    !mv {fasta_dir}/*.fai {fai_dir}\n",
    "    fasta_path=fasta_dir + '/' + fasta_name\n",
    "    if not os.path.splitext(fasta_path)[1] == 'fai':\n",
    "        print('Now collecting dataset for: ' + fasta_path)\n",
    "        dataset_list.append(makeEnformerDataset(fasta_path, hg38_bed_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1e4868-9531-45ec-abdb-2ceaf3e84366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDatasets(dataset_list):\n",
    "    '''\n",
    "        uses hf datasets to concatenate a list of datasets\n",
    "    '''\n",
    "    return concatenate_datasets(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65306cf8-4d3b-4a27-b442-0858ea32c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combineDatasets(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88301ea-7268-4442-b20d-0ab2cb59e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.save_to_disk(bam_dir + '/' + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524f3d4-6d96-4d7f-a828-94c98903ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainEnformerPTA(combined_datset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e1357-1b47-4853-b017-adedf9bf2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ### enter a sequence to test\n",
    "pred = model(seq, head = 'human') # (896, 5313) ###<- what does human mean here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
