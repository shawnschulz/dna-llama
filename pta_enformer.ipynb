{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f5e25df-aca9-41c1-b9f4-e592b005f460",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663fa7f8-5f83-40c3-9792-882c40b9312a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/shawn/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/shawn/.local/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/shawn/.local/lib/python3.10/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: sympy in /home/shawn/.local/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /home/shawn/.local/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/shawn/.local/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/shawn/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.1)\n",
      "Requirement already satisfied: lit in /home/shawn/.local/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/shawn/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/shawn/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bc3f67-cc87-4ac8-8ed1-d6a2ac1f10a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/shawn/.local/lib/python3.10/site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/shawn/.local/lib/python3.10/site-packages (from transformers) (1.24.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shawn/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b69fb5-19ea-4cab-96b5-22fb99f04f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/shawn/.local/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (1.24.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: packaging in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: responses<0.19 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: pandas in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: xxhash in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: multiprocess in /home/shawn/.local/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/shawn/.local/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/shawn/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /home/shawn/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.10.7)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /home/shawn/.local/lib/python3.10/site-packages (from responses<0.19->datasets) (1.26.15)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/shawn/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/shawn/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c502e4-3bbf-4106-ba39-08b36abb817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pysam in /home/shawn/.local/lib/python3.10/site-packages (0.20.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pysam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc08e69-7d8d-4368-805a-3d07968804f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: HTSeq in /home/shawn/.local/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: pysam in /home/shawn/.local/lib/python3.10/site-packages (from HTSeq) (0.20.0)\n",
      "Requirement already satisfied: numpy in /home/shawn/.local/lib/python3.10/site-packages (from HTSeq) (1.24.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install HTSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc79d57-331b-4894-b306-7ea092a926f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install enformer-pytorch>=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba1ede2-ccd6-433d-aed3-b7df13be183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: polars in /home/shawn/.local/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0.1 in /home/shawn/.local/lib/python3.10/site-packages (from polars) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4734ba7-c0d4-4f4e-8877-13a88bfcd174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/shawn/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/shawn/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/shawn/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/shawn/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/shawn/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02d7ef-b040-4419-b0d0-c4a8178e6b84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Module imports and variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1079afb-7b2c-4833-bc76-621e0f397109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "from enformer_pytorch import Enformer, GenomeIntervalDataset\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5a7935-f1d4-402e-89d4-583fcba9ed3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### define variables\n",
    "hg38_bed_path = \"/scratch/users/sschulz/pta_on_normal/Homo_sapiens_assembly38_n25chr.bed\"\n",
    "bam_dir= \"/scratch/users/sschulz/pta_on_normal\"\n",
    "fasta_dir = bam_dir + '/fastas'\n",
    "dataset_name='test_dataset.hf'\n",
    "preprocess_bam=True\n",
    "vcf_file=bam_dir +'/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz'\n",
    "ref_fasta=bam_dir + '/Homo_sapiens_assembly38.fasta'\n",
    "results_dir=bam_dir\n",
    "output_fasta_fn='chr_test_fasta.fasta'\n",
    "sbatch=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7229ab-99f5-4d74-a1e0-2a5cf0c47db1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366bf928-a137-44bb-b33e-39616a80d8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preProcessVCF(vcf_path, ref_fasta, results_dir, output_fasta_fn, sbatch):\n",
    "    '''\n",
    "        take directory of a combined vcf file with somatic mutations called for normal cells\n",
    "        with pta run on them and make a fasta file \n",
    "        \n",
    "        TO-DO: make something that generates the vcf for you if given normal bam files\n",
    "    '''\n",
    "\n",
    "    !sh vcf2fasta.sh --vcf_path $vcf_path --ref_fasta $ref_fasta --results_dir $results_dir --output_name $output_fasta_fn --sbatch $sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbddb491-ee04-4717-bceb-edd694f04f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEnformerDataset(fasta_path, bed_path):\n",
    "    '''\n",
    "        makes enformer dataset for genome intervals in bed file at bed_path, returns the dataset\n",
    "    '''\n",
    "    #filter_train = lambda df: df.filter(pl.col('column_4') == 'train')\n",
    "    ds = GenomeIntervalDataset(\n",
    "        bed_file = bed_path, ##<- this can just be the whole hg38.bed for all chromosomes\n",
    "        fasta_file = fasta_path,  ## path to fasta file\n",
    "        #filter_df_fn = filter_train,                        # filter dataframe function\n",
    "        return_seq_indices = False,                          # return nucleotide indices (ACGTN) or one hot encodings\n",
    "        shift_augs = (-2, 2),                               # random shift augmentations from -2 to +2 basepairs\n",
    "        rc_aug = True,                                      # use reverse complement augmentation with 50% probability\n",
    "        context_length = 196_608,\n",
    "        return_augs = True                                  # return the augmentation meta data\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a767409c-b1a4-4a25-9375-476c72ac0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEnformerPTA(dataset):\n",
    "    '''\n",
    "        should train the enformer model\n",
    "    '''\n",
    "    \n",
    "    #filter_train = lambda df: df.filter(pl.col('column_4') == 'train')\n",
    "\n",
    "    model = HeadAdapterWrapper(\n",
    "                enformer = dataset,\n",
    "                num_tracks = 128,\n",
    "                post_transformer_embed = False   # by default, embeddings are taken from after the final pointwise block w/ conv -> gelu - but if you'd like the embeddings right after the transformer block with a learned layernorm, set this to True\n",
    "            ).cuda()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7b7d85-dea8-4e4e-b365-3ef6d9909738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOneHot(fastaFilePath, num_lines):\n",
    "    '''\n",
    "       Give path to a fasta file and the number of lines you want to turn into one hot encodings, get the one_hot encoded\n",
    "       tensor\n",
    "    '''\n",
    "    with open(fastaFilePath, 'r') as input_file:\n",
    "        try:\n",
    "            head = [next(input_file).strip('\\n') for _ in range(num_lines)]\n",
    "        except StopIteration as e:\n",
    "            print(\"Exceeded length of input file, stopping.\")\n",
    "    seq = str_to_one_hot(head)\n",
    "    input_file.close()\n",
    "    return(seq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be1b5d5-2505-4618-8e6c-0068f2c8850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this was all taken from lucidrains/enformer-pytorch, i'll import this properly later \n",
    "\n",
    "def torch_fromstring(seq_strs):\n",
    "    def cast_list(t):\n",
    "        return t if isinstance(t, list) else [t]\n",
    "    batched = not isinstance(seq_strs, str)\n",
    "    seq_strs = cast_list(seq_strs)\n",
    "    np_seq_chrs = list(map(lambda t: np.fromstring(t, dtype = np.uint8), seq_strs))\n",
    "    seq_chrs = list(map(torch.from_numpy, np_seq_chrs))\n",
    "    return torch.stack(seq_chrs) if batched else seq_chrs[0]\n",
    "\n",
    "def str_to_one_hot(seq_strs):\n",
    "    seq_chrs = torch_fromstring(seq_strs)\n",
    "    return one_hot_embed[seq_chrs.long()]\n",
    "\n",
    "one_hot_embed = torch.zeros(256, 4)\n",
    "one_hot_embed[ord('a')] = torch.Tensor([1., 0., 0., 0.])\n",
    "one_hot_embed[ord('c')] = torch.Tensor([0., 1., 0., 0.])\n",
    "one_hot_embed[ord('g')] = torch.Tensor([0., 0., 1., 0.])\n",
    "one_hot_embed[ord('t')] = torch.Tensor([0., 0., 0., 1.])\n",
    "one_hot_embed[ord('n')] = torch.Tensor([0., 0., 0., 0.])\n",
    "one_hot_embed[ord('A')] = torch.Tensor([1., 0., 0., 0.])\n",
    "one_hot_embed[ord('C')] = torch.Tensor([0., 1., 0., 0.])\n",
    "one_hot_embed[ord('G')] = torch.Tensor([0., 0., 1., 0.])\n",
    "one_hot_embed[ord('T')] = torch.Tensor([0., 0., 0., 1.])\n",
    "one_hot_embed[ord('N')] = torch.Tensor([0., 0., 0., 0.])\n",
    "one_hot_embed[ord('.')] = torch.Tensor([0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7e272-f2c9-42ef-9bcc-cf1dcb9fe831",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8293b8-faf6-4277-9cba-c42c52693b22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## preprocessing the target fasta\n",
    "bam_dir='/home/shawn/datasets/enformer_data'\n",
    "vcf_file=bam_dir +'/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz'\n",
    "ref_fasta=bam_dir + '/Homo_sapiens_assembly38.fasta'\n",
    "results_dir=bam_dir\n",
    "output_fasta_fn='final_fasta.fasta'\n",
    "preprocess_bam=False\n",
    "if preprocess_bam:\n",
    "    preProcessVCF(vcf_file, ref_fasta, results_dir, output_fasta_fn, sbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35f89d49-d108-44f4-8d81-d38a9f1db215",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing the input fasta\n",
    "vcf_file=bam_dir +'/CARTPt04_Scan2_svc_merged_extract_snp.vcf.gz'\n",
    "ref_fasta=bam_dir + '/Homo_sapiens_assembly38.fasta'\n",
    "results_dir=bam_dir\n",
    "output_fasta_fn='test_fasta.fasta'\n",
    "if preprocess_bam:\n",
    "    preProcessVCF(vcf_file, ref_fasta, results_dir, output_fasta_fn, sbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506a144-6c45-4ec9-abf4-5b6bdf94ec92",
   "metadata": {},
   "source": [
    "## get one-hot encodings with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e8c54d-1b42-4d3b-b253-6045659d6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_438552/3370287454.py:8: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  np_seq_chrs = list(map(lambda t: np.fromstring(t, dtype = np.uint8), seq_strs))\n"
     ]
    }
   ],
   "source": [
    "lines_of_fasta_file=3000\n",
    "targetFastaFilePath = bam_dir + '/final_fasta.fasta'\n",
    "\n",
    "target = makeOneHot(targetFastaFilePath, lines_of_fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c6475a-efb6-4ccf-a482-991fb45074c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_438552/3370287454.py:8: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  np_seq_chrs = list(map(lambda t: np.fromstring(t, dtype = np.uint8), seq_strs))\n"
     ]
    }
   ],
   "source": [
    "lines_of_fasta_file=3000\n",
    "inputFastaFilePath = bam_dir + '/test_fasta.fasta'\n",
    "\n",
    "input_seq = makeOneHot(targetFastaFilePath, lines_of_fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7b83a01-60c4-47b3-a023-448e3bc0ccb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_seq = input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf89ffc-e6f7-4307-b2ea-f86974184bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63a45bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 60, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18978368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 60, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7ae09-eb18-4b7d-8503-e1f9a37e8e63",
   "metadata": {},
   "source": [
    "## Run the model on new DNA tracks with somatic mutations detected in normal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763fc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from enformer_pytorch import Enformer, seq_indices_to_one_hot\n",
    "\n",
    "# will need to figure out correct way to configure hyperparams\n",
    "model = Enformer.from_hparams(\n",
    "    dim = 1536,\n",
    "    depth = 11,\n",
    "    heads = 4,\n",
    "    output_heads = dict(human = 4), #<- tbh idk what the correct number of output heads should be\n",
    "    target_length = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4147e70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Enformer(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): Residual(\n",
       "      (fn): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): AttentionPool(\n",
       "      (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "      (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (conv_tower): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (transformer): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): Sequential(\n",
       "      (0): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Attention(\n",
       "            (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "            (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "            (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "            (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "          (2): Dropout(p=0.4, inplace=False)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "          (5): Dropout(p=0.4, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (crop_final): TargetLengthCrop()\n",
       "  (final_pointwise): Sequential(\n",
       "    (0): Rearrange('b n d -> b d n')\n",
       "    (1): Sequential(\n",
       "      (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): GELU()\n",
       "      (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (2): Rearrange('b d n -> b n d')\n",
       "    (3): Dropout(p=0.05, inplace=False)\n",
       "    (4): GELU()\n",
       "  )\n",
       "  (_trunk): Sequential(\n",
       "    (0): Rearrange('b n d -> b d n')\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Rearrange('b d n -> b n d')\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=256, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=384, out_features=256, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TargetLengthCrop()\n",
       "    (6): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Rearrange('b d n -> b n d')\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): GELU()\n",
       "    )\n",
       "  )\n",
       "  (_heads): ModuleDict(\n",
       "    (human): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=4, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a19efb5d-f255-47fa-9ecb-ade7f30d910c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    target should EITHER be the one hot encodings of an input fasta with many artifacts, or the weights from \\n    just running the model on the one hot encodings without a target, not exactly sure which but try the \\n    one hot encodigns first\\n    \\n    input_seq should be the one-hot embeddings of any fasta\\n    \\n    corr_coef should be higher if many artifacts, lower if not many artifacts. check against a bulk sequencing sample,\\n    another fasta that should have normal artifacts in it, and a single cell DNA sequencing sample\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, embeddings = model(input_seq, return_embeddings = True)\n",
    "\n",
    "'''\n",
    "    target should EITHER be the one hot encodings of an input fasta with many artifacts, or the weights from \n",
    "    just running the model on the one hot encodings without a target, not exactly sure which but try the \n",
    "    one hot encodigns first\n",
    "    \n",
    "    input_seq should be the one-hot embeddings of any fasta\n",
    "    \n",
    "    corr_coef should be higher if many artifacts, lower if not many artifacts. check against a bulk sequencing sample,\n",
    "    another fasta that should have normal artifacts in it, and a single cell DNA sequencing sample\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fae4bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': tensor([[[0.5952, 0.7695, 0.7897, 0.6476]],\n",
       " \n",
       "         [[0.6267, 0.6530, 0.6533, 0.8071]],\n",
       " \n",
       "         [[0.7138, 0.7319, 0.7397, 0.6548]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.5501, 0.7493, 0.7251, 0.6794]],\n",
       " \n",
       "         [[0.6289, 0.5940, 0.8572, 0.6837]],\n",
       " \n",
       "         [[0.6221, 0.6564, 0.7102, 0.6964]]], grad_fn=<SoftplusBackward0>)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c61fbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0268, -0.1301,  0.0757,  ..., -0.1336, -0.0000, -0.1507]],\n",
       "\n",
       "        [[-0.0210, -0.1289,  0.0524,  ...,  0.0125, -0.1299, -0.1050]],\n",
       "\n",
       "        [[ 0.2308, -0.1610, -0.0126,  ...,  0.1239,  0.1856,  0.0499]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0228, -0.1304,  0.0126,  ...,  0.1130, -0.1551, -0.0362]],\n",
       "\n",
       "        [[ 0.5090,  0.1449, -0.0137,  ...,  0.1015, -0.0358, -0.0000]],\n",
       "\n",
       "        [[-0.1093,  0.4288, -0.0807,  ..., -0.1275, -0.1449,  0.0471]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TODO should put the embeddings in a vector database\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "705d5aa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "loss, embeddings = model(\n",
    "    target,\n",
    "    head = 'human',\n",
    "    target = output['human'],\n",
    "    return_embeddings = True\n",
    ")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# after much training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728d276c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_coef = model(\n",
    "    input_seq,\n",
    "    head = 'human',\n",
    "    target = output['human'],\n",
    "    return_corr_coef = True\n",
    ")\n",
    "\n",
    "corr_coef # pearson R, used as a metric in the paper. For our first test, this should return something close to 1, since \n",
    "          # we are running it on the exact same fasta file as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d24d01b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output[\u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "output['human'].backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1142a45-1dd7-4798-a956-12080533b04e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run the model with finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea8b29-7eab-4356-bab1-7d8a457542ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from enformer_pytorch import Enformer\n",
    "from enformer_pytorch.finetune import HeadAdapterWrapper\n",
    "\n",
    "enformer = Enformer.from_pretrained('EleutherAI/enformer-official-rough')\n",
    "\n",
    "model = HeadAdapterWrapper(\n",
    "    enformer = enformer,\n",
    "    num_tracks = 128,\n",
    "    post_transformer_embed = False   # by default, embeddings are taken from after the final pointwise block w/ conv -> gelu - but if you'd like the embeddings right after the transformer block with a learned layernorm, set this to True\n",
    ")\n",
    "\n",
    "target = torch.randn(1, 1, 128).cuda()  # 128 tracks\n",
    "\n",
    "loss = model(input_seq, target = target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd56850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadAdapterWrapper(\n",
       "  (enformer): Enformer(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): AttentionPool(\n",
       "        (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "        (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (conv_tower): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): Sequential(\n",
       "        (0): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "              (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "              (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "              (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            )\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "            (2): Dropout(p=0.4, inplace=False)\n",
       "            (3): ReLU()\n",
       "            (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "            (5): Dropout(p=0.4, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (crop_final): TargetLengthCrop()\n",
       "    (final_pointwise): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (2): Rearrange('b d n -> b n d')\n",
       "      (3): Dropout(p=0.05, inplace=False)\n",
       "      (4): GELU()\n",
       "    )\n",
       "    (_trunk): Sequential(\n",
       "      (0): Rearrange('b n d -> b d n')\n",
       "      (1): Sequential(\n",
       "        (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "        (1): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "        )\n",
       "        (2): AttentionPool(\n",
       "          (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "          (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(896, 896, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1152, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): GELU()\n",
       "            (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "              (1): GELU()\n",
       "              (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): AttentionPool(\n",
       "            (pool_fn): Rearrange('b d (n p) -> b d n p', p=2)\n",
       "            (to_attn_logits): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Rearrange('b d n -> b n d')\n",
       "      (4): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Attention(\n",
       "                (to_q): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_k): Linear(in_features=1536, out_features=512, bias=False)\n",
       "                (to_v): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "                (to_out): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (to_rel_k): Linear(in_features=192, out_features=512, bias=False)\n",
       "                (pos_dropout): Dropout(p=0.01, inplace=False)\n",
       "                (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=1536, out_features=3072, bias=True)\n",
       "              (2): Dropout(p=0.4, inplace=False)\n",
       "              (3): ReLU()\n",
       "              (4): Linear(in_features=3072, out_features=1536, bias=True)\n",
       "              (5): Dropout(p=0.4, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): TargetLengthCrop()\n",
       "      (6): Sequential(\n",
       "        (0): Rearrange('b n d -> b d n')\n",
       "        (1): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 3072, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (2): Rearrange('b d n -> b n d')\n",
       "        (3): Dropout(p=0.05, inplace=False)\n",
       "        (4): GELU()\n",
       "      )\n",
       "    )\n",
       "    (_heads): ModuleDict(\n",
       "      (human): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=5313, bias=True)\n",
       "        (1): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (mouse): Sequential(\n",
       "        (0): Linear(in_features=3072, out_features=1643, bias=True)\n",
       "        (1): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_embed_transform): Sequential(\n",
       "    (0): Identity()\n",
       "  )\n",
       "  (to_tracks): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=128, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d915-3267-40f7-8b23-4cb1c49b82d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HeadAdapterWrapper.forward() got an unexpected keyword argument 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[39m=\u001b[39m model(input_seq, head \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# (3000, 3000)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: HeadAdapterWrapper.forward() got an unexpected keyword argument 'head'"
     ]
    }
   ],
   "source": [
    "pred = model(input_seq, head = 'human') # (3000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec0ffa-c886-4a84-ba4c-e05c65a60fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7276, 0.7898, 0.7651,  ..., 0.8316, 0.6573, 0.5948]],\n",
       "\n",
       "        [[0.7207, 0.7824, 0.8117,  ..., 0.7090, 0.5889, 0.5795]],\n",
       "\n",
       "        [[0.5686, 0.8286, 0.8646,  ..., 0.7037, 0.6471, 0.6555]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.7175, 0.7805, 0.7743,  ..., 0.7038, 0.8123, 0.5317]],\n",
       "\n",
       "        [[0.6933, 0.6910, 0.7489,  ..., 0.7125, 0.6352, 0.6702]],\n",
       "\n",
       "        [[0.6963, 0.7716, 0.8335,  ..., 0.6973, 0.6662, 0.6498]]],\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffcc18d-6460-4a88-811b-904883888cdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d03666-0dca-4570-bffa-f849dba480d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat ‘/scratch/users/sschulz/pta_on_normal/fastas/*.fai’: No such file or directory\n",
      "Now collecting dataset for: /scratch/users/sschulz/pta_on_normal/fastas/CART-MRD-BALL-PTA-NEXTERA-WGS-CCT5007Pt04-A9_S57.realigned_deduped_sorted.fasta\n",
      "Now collecting dataset for: /scratch/users/sschulz/pta_on_normal/fastas/CART-MRD-BALL-PTA-NEXTERA-WGS-CCT5007Pt04-B10_S65.realigned_deduped_sorted.fasta\n"
     ]
    }
   ],
   "source": [
    "dataset_list = []\n",
    "fai_dir=fasta_dir + '/fais'\n",
    "!mkdir -p {fai_dir}\n",
    "for fasta_name in os.listdir(fasta_dir):\n",
    "    !mv {fasta_dir}/*.fai {fai_dir}\n",
    "    fasta_path=fasta_dir + '/' + fasta_name\n",
    "    if not os.path.splitext(fasta_path)[1] == 'fai':\n",
    "        print('Now collecting dataset for: ' + fasta_path)\n",
    "        dataset_list.append(makeEnformerDataset(fasta_path, hg38_bed_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e4868-9531-45ec-abdb-2ceaf3e84366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDatasets(dataset_list):\n",
    "    '''\n",
    "        uses hf datasets to concatenate a list of datasets\n",
    "    '''\n",
    "    return concatenate_datasets(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65306cf8-4d3b-4a27-b442-0858ea32c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = combineDatasets(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88301ea-7268-4442-b20d-0ab2cb59e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.save_to_disk(bam_dir + '/' + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524f3d4-6d96-4d7f-a828-94c98903ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainEnformerPTA(combined_datset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e1357-1b47-4853-b017-adedf9bf2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = ### enter a sequence to test\n",
    "pred = model(seq, head = 'human') # (896, 5313) ###<- what does human mean here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
